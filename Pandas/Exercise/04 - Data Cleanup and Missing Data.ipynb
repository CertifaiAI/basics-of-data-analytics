{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "massive-geometry",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## ![logo](../../img/license_header_logo.png)\n",
    "> **Copyright &copy; 2021 CertifAI Sdn. Bhd.**<br>\n",
    " <br>\n",
    "This program is part of OSRFramework. You can redistribute it and/or modify\n",
    "<br>it under the terms of the GNU Affero General Public License as published by\n",
    "<br>the Free Software Foundation, either version 3 of the License, or\n",
    "<br>(at your option) any later version.\n",
    "<br>\n",
    "<br>This program is distributed in the hope that it will be useful,\n",
    "<br>but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "<br>MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "<br>GNU Affero General Public License for more details.\n",
    "<br>\n",
    "<br>You should have received a copy of the GNU Affero General Public License\n",
    "<br>along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-waters",
   "metadata": {},
   "source": [
    "# <a name=\"top\">04 - Data Cleanup and Missing Data</a>\n",
    "Authored by: Scotrraaj Gopal - scotrraaj.gopal@certifai.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-perspective",
   "metadata": {},
   "source": [
    "## <a name=\"description\">Notebook Description</a>\n",
    "\n",
    "When you get out of the classroom and start applying your analytical skills in the real world, you'll realise that the dataset that you obtain is not really like how you had expected. Datasets go through varieties of processes before reaching your hands, causing it to end up being unstructured and rather 'unfriendly' for the data scientist. This tutorial focuses on how to clean up your data and a handful of methods in  dealing with missing data.\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. List, rename and delete columns.\n",
    "2. Detect and count `NaN` values\n",
    "3. Remove rows with `NaN` values.\n",
    "4. Replace `NaN` values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-lancaster",
   "metadata": {},
   "source": [
    "## Notebook Outline\n",
    "Here's the outline for this tutorial:\n",
    "1. [Notebook Description](#description)\n",
    "2. [Notebook Configurations](#configuration)\n",
    "3. [Data Cleanup](#clean)\n",
    "    - [Handling Duplicates](#duplicates)\n",
    "    - [Column Formatting](#columns)\n",
    "4. [Missing Data](#missing)\n",
    "    - [How Would You Know?](#detect)\n",
    "    - [What To Do?](#action)\n",
    "5. [Summary](#summary)\n",
    "6. [Reference](#reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-nickel",
   "metadata": {},
   "source": [
    "## <a name=\"configuration\">Notebook Configurations</a>\n",
    "\n",
    "Begin by first importing the `pandas` module and the dataset from `../../Datasets/pandas/winemag-data-130k-v2.csv`. Name the `DataFrame` object `reviews`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-bidding",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d93eb48ac391bf48ccb8a2c60438d7c",
     "grade": false,
     "grade_id": "cell-826f6d99baefdbd6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a58644",
   "metadata": {},
   "source": [
    "## <a name=\"clean\">Data Cleanup</a>\n",
    "\n",
    "This is a pre-processing step that has to be done before beginning to analyse the dataset. Usually the pre-processing step is the most extensive and challenging one because it has to be done carefully without introducing bias.\n",
    "\n",
    "### <a name=\"duplicates\">Handling Duplicates</a>\n",
    "\n",
    "It is always important to verify that the dataset that we are working with has no duplicates, so that we know for sure that we are not aggregating duplicate rows. Removing duplicates can be considered the lowest fruit to pluck when it comes to preprocessing data.\n",
    "\n",
    "We can do this with the `.duplicated()` method. Chaining it with the `.sum()` shows the total duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990ede97",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f27dc7b4e817b6c6478e4e57f799ffab",
     "grade": false,
     "grade_id": "cell-d00052949851d4e9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40835348",
   "metadata": {},
   "source": [
    "There are 9,983 duplicate rows in our dataset. Let's use the `.drop_duplicates()` method to drop the duplicates.\n",
    "\n",
    "> *Note: Use the* `inplace` *flag once you are sure with the operation as this would immediately affect the root dataset variable. __Be cautious when using this flag as mistakes of using the flag on a big dataset can be costly in terms of time and effort.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dc7207",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82df7369a51b5a8999db1b35bb7cb6b1",
     "grade": false,
     "grade_id": "cell-86508d003120641d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Shape of reviews with duplicates = {reviews.shape}\")\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "print(f\"Shape of reviews without duplicates = {reviews.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe887046",
   "metadata": {},
   "source": [
    "### <a name=\"columns\">Column Formatting</a>\n",
    "\n",
    "Routinely, datasets will have unstructured column labels with some of them being a cocktail of lowercase and uppercase words, spaces and typos. In order to make our life easier when selecting data by columns, we can spend a little time on cleaning up their names.\n",
    "\n",
    "We can access the column labels, all at once, with `.columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba0f93",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ace355799fc509b329ff9b9665f76f23",
     "grade": false,
     "grade_id": "cell-d0e220c6a28147af",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d121ba",
   "metadata": {},
   "source": [
    "We can use the `.rename()` method to rename certain columns with a `dict` style argument.\n",
    "\n",
    "Let's rename the `points` column to `score` and `taster_twitter_handle` to `taster_twitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b5fdb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f60562f03f28cc9fc3f899871ea0021",
     "grade": false,
     "grade_id": "cell-fe1472393a798d07",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "reviews.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07953b33",
   "metadata": {},
   "source": [
    "The column labels of our dataset is actually already in good shape. The best practices for formatting column labels are as follows:\n",
    "1. Lowercase letters\n",
    "2. No special character such as symbols and brackets\n",
    "3. Spaces replaced with underscores\n",
    "4. Short but descriptive\n",
    "\n",
    "Now, what if there are columns that doesn't bring any meaning to your problem statement?\n",
    "\n",
    "You can always free up some space and clutter in your dataset by using `.drop()` method. This is a powerful method to remove rows or colums. Use `axis` attribute to specify if you're removing a column (`axis=1`) or a row (`axis=0`)\n",
    "\n",
    "Let's remove the `taster_twitter` column from our `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c223eea",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c9df9f2abad60ae35a38a33e69e4c74",
     "grade": false,
     "grade_id": "cell-64cf700a7e4f4680",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"reviews columns before dropping: {reviews.columns}\")\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "print(f\"reviews columns after dropping: {reviews.columns}\")\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f0867f",
   "metadata": {},
   "source": [
    "## <a name=\"missing\">Missing Data</a>\n",
    "\n",
    "When investigating your data, you will most inevitably come across missing or null values which are generally placeholders for non-existent information. By `pandas` default, missing values in a dataset are given the values `NaN`, short for \"Not a Number\". \n",
    "\n",
    "### <a name=\"detect\">How Would You Know?</a>\n",
    "\n",
    "While you can always inspect your dataset by eyeballing out every occurence of `NaN` and dealing with them, this may not be very feasible when you have thousands of rows of data.\n",
    "\n",
    "You can check if your `DataFrame` object has `NaN` with the method `.isna()`. Chain it with `.sum()` to obtain a total of `NaN` values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249d362e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46e3b1f811cc8ee6275040f3b3f96dd0",
     "grade": false,
     "grade_id": "cell-bf969b77ead5d2e7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b2e6b8",
   "metadata": {},
   "source": [
    "### <a name='action'>What To Do?</a>\n",
    "\n",
    "There are a two strategies that can be done when encountered with `NaN` values. Deciding on which strategy to go with requires intimate knowledge of the dataset and its context.\n",
    "\n",
    "1. Remove the whole row with `NaN` values.\n",
    "2. Use data imputation to fill the `NaN` values with a reasonably justified value.\n",
    "\n",
    "#### Removing the rows with `NaN` values is rather straightforward. \n",
    "\n",
    "Use the `.dropna()` method to return a version of the `DataFrame` without any `NaN` values. Since we still need the `NaN` values to showcase the next strategy, we will not use the `inplace` attribute. Chain the `.reset_index()` method with appropriate attributes *(wink.. wink..)* to provide a new index flow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0336f24e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7083a09aae36554ccecd32538cd1e68c",
     "grade": false,
     "grade_id": "cell-bf0708ce509d3be4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948f66ee",
   "metadata": {},
   "source": [
    "#### Replace `NaN` values with data imputation.\n",
    "\n",
    "Replacing missing values is a conventional operation to keep valuable data that have `NaN` values. We can opt into this strategy when dropping every row with missing data causes the lost of a huge chunk of data. `pandas` provide a really handy method for this problem: `.fillna()`. This method allows a few different ways of replacing the values to mitigate such data.\n",
    "\n",
    "For example, we can simply replace the `NaN` values in every row with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e83acd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38dcf2640ee860358eb861d329c3dfcd",
     "grade": false,
     "grade_id": "cell-b982d764b231c00e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7079dfcd",
   "metadata": {},
   "source": [
    "There is also another method called `.replace()` that can be used to deal with this issue. But this method is more versatile in its uses as it can be utilized to even replace non-`NaN` values. \n",
    "\n",
    "For example, let's say the element in the `variety` column called `White Blend` has been updated recently to `White Mix`. We can easily implement this change in our dataset with `.replace()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da2ff91",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e553fe1df594f92d55000bd71f3a4ac",
     "grade": false,
     "grade_id": "cell-eb898559c0095e95",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"First row in 'variety' column before replace: {reviews.variety[0]}\")\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "print(f\"First row in 'variety' column after replace: {reviews.variety[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b877299f",
   "metadata": {},
   "source": [
    "We may use `.replace()` to replace `NaN` values with values that can be more relatable to the data. For example, `NaN` values in the `price` column can be replaced with `Free` or the average price etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14661d4",
   "metadata": {},
   "source": [
    "##  <a name=\"summary\">Summary</a>\n",
    "To conclude, you should now be able to:\n",
    "\n",
    "1. List, rename and delete columns.\n",
    "2. Detect and count `NaN` values\n",
    "3. Remove rows with `NaN` values.\n",
    "4. Replace `NaN` values.\n",
    "\n",
    "Congratulations, that concludes this tutorial. Exploring, cleaning and transforming data is an essential skill in data science. After some practice, you should be really comfortable with most of the basics. \n",
    "\n",
    "In your learning journey, you may come across errors in many different forms. Don't let that discourage you as even the best programmers have them too. Steer through the error by interpreting it and try your level best to debug your code. You may also use this [guide](https://geo-python.github.io/site/notebooks/L6/errors.html) as a point of reference.\n",
    "\n",
    "There is an exercise notebook in this folder that can be helpful for you to test out everything that you've learnt. Don't forget to check it out!\n",
    "\n",
    "**See you and happy coding!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-protection",
   "metadata": {},
   "source": [
    "## <a name=\"reference\">Reference</a>\n",
    "* [Dataset Source](https://www.kaggle.com/zynicide/wine-reviews)\n",
    "\n",
    "<font size=2>[Back to Top](#top)</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}